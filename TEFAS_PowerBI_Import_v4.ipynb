{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+wSpP1Chf4WFu1GLQHbPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozturkergin/ozturkergin/blob/main/TEFAS_PowerBI_Import_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  pip install bs4 --quiet"
      ],
      "metadata": {
        "id": "pmX_SWi-tcz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install urllib3 --quiet"
      ],
      "metadata": {
        "id": "qG7wEa-tKBUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install marshmallow --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNJryUo7KCgC",
        "outputId": "6bdb9c6b-4909-43bc-8eca-978612491fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m41.0/49.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m774.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas_ta --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0jy0-6JdKnQ",
        "outputId": "3ec2fcf5-2412-431f-defe-499c8738e635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/115.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/115.1 kB\u001b[0m \u001b[31m848.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install free-proxy --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVp0PoucVHuV",
        "outputId": "704ba64a-776e-408a-887d-ca14eb8e81bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import math\n",
        "import concurrent.futures\n",
        "import time\n",
        "\n",
        "from datetime import datetime, timedelta, date\n",
        "from typing import Dict, List, Optional, Union\n",
        "from marshmallow import Schema, fields, EXCLUDE, pre_load, post_load\n",
        "from bs4 import BeautifulSoup\n",
        "from fp.fp import FreeProxy\n",
        "\n",
        "# Special thanks to https://github.com/burakyilmaz321\n",
        "\n",
        "class InfoSchema(Schema):\n",
        "    code = fields.String(data_key=\"FONKODU\", allow_none=True)\n",
        "    fonunvantip = fields.String(data_key=\"FONUNVANTIP\", allow_none=True)\n",
        "    date = fields.Date(data_key=\"TARIH\", allow_none=True)\n",
        "    price = fields.Float(data_key=\"FIYAT\", allow_none=True)\n",
        "    title = fields.String(data_key=\"FONUNVAN\", allow_none=True)\n",
        "    market_cap = fields.Float(data_key=\"PORTFOYBUYUKLUK\", allow_none=True)\n",
        "    number_of_shares = fields.Float(data_key=\"TEDPAYSAYISI\", allow_none=True)\n",
        "    number_of_investors = fields.Float(data_key=\"KISISAYISI\", allow_none=True)\n",
        "\n",
        "    @pre_load\n",
        "    def pre_load_hook(self, input_data, **kwargs):\n",
        "        seconds_timestamp = int(input_data[\"TARIH\"]) / 1000\n",
        "        input_data[\"TARIH\"] = date.fromtimestamp(seconds_timestamp).isoformat()\n",
        "        return input_data\n",
        "\n",
        "    @post_load\n",
        "    def post_load_hool(self, output_data, **kwargs):\n",
        "        output_data = {f: output_data.setdefault(f) for f in self.fields}\n",
        "        return output_data\n",
        "\n",
        "    class Meta:\n",
        "        unknown = EXCLUDE\n",
        "\n",
        "class tefas_get:\n",
        "    root_url = \"https://www.tefas.gov.tr\"\n",
        "    info_endpoint = \"/api/DB/BindHistoryInfo\"\n",
        "    concurrently = False\n",
        "    use_Proxy = False\n",
        "    fon_type = \"YAT\"\n",
        "    proxies = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_combobox_items(url, select_id):\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Failed to fetch the URL: {response.status_code}\")\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        select_element = soup.find('select', id=select_id)\n",
        "\n",
        "        if not select_element:\n",
        "            raise Exception(f\"Select element with id '{select_id}' not found\")\n",
        "\n",
        "        options = select_element.find_all('option')\n",
        "        options = list(filter(None, options))\n",
        "\n",
        "        items = []\n",
        "        for option in options:\n",
        "            value = option.get('value')\n",
        "            items.append(value)\n",
        "\n",
        "        items.remove('')\n",
        "\n",
        "        return items\n",
        "\n",
        "    def fetch_info(self, fonunvantip, start_date_initial, end_date_initial):\n",
        "        counter = 1\n",
        "        start_date = start_date_initial\n",
        "        end_date = end_date_initial\n",
        "        range_date = end_date_initial - start_date_initial\n",
        "        range_interval = 90\n",
        "        info_schema = InfoSchema(many=True)\n",
        "        info_result = pd.DataFrame()\n",
        "\n",
        "        if range_date.days > range_interval :\n",
        "            counter = range_date.days / range_interval\n",
        "            counter = math.ceil(counter)\n",
        "            end_date = start_date + timedelta(days=range_interval)\n",
        "\n",
        "        while counter > 0:\n",
        "            counter -= 1\n",
        "\n",
        "            data = {\n",
        "                    \"fontip\": self.fon_type,\n",
        "                    \"bastarih\": self._parse_date(start_date),\n",
        "                    \"bittarih\": self._parse_date(end_date),\n",
        "                    \"fonunvantip\": fonunvantip,\n",
        "                    \"fonkod\": \"\",\n",
        "                  }\n",
        "\n",
        "            info = self._do_post(data)\n",
        "            info = info_schema.load(info)\n",
        "            info = pd.DataFrame(info, columns=info_schema.fields.keys())\n",
        "            #info['fonunvantip'] = fonunvantip.replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\")\n",
        "            info['fonunvantip'] = fonunvantip\n",
        "            #info = info[info['price'] != 0]\n",
        "            if not info.empty :\n",
        "                info_result = pd.concat([info_result, info])\n",
        "                info_result = info_result.reset_index(drop=True)\n",
        "                info = info.reset_index(drop=True)\n",
        "\n",
        "            if counter > 0 :\n",
        "                start_date = end_date + timedelta(days=1)\n",
        "                end_date = end_date + timedelta(days=range_interval)\n",
        "                if end_date > end_date_initial :\n",
        "                    end_date = end_date_initial\n",
        "\n",
        "        return info_result\n",
        "\n",
        "    def fetch_info_serial(self, fonunvantips, start_date_initial, end_date_initial):\n",
        "        merged = pd.DataFrame()\n",
        "\n",
        "        for fonunvantip in fonunvantips:\n",
        "            info = self.fetch_info(fonunvantip, start_date_initial, end_date_initial)\n",
        "            if not info.empty :\n",
        "                merged = pd.concat([merged, info])\n",
        "                print(f\"{fonunvantip} - {len(info)} records added total records: {len(merged)} \" )\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def fetch_info_concurrently(self, fonunvantips, start_date_initial, end_date_initial):\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
        "            merged = pd.DataFrame()\n",
        "            # Submit all tasks to the executor\n",
        "            self.concurrently = True\n",
        "            futures = {executor.submit(self.fetch_info, fonunvantip, start_date_initial, end_date_initial): fonunvantip for fonunvantip in fonunvantips}\n",
        "\n",
        "            # Retrieve results as they complete\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                info = future.result()\n",
        "                merged = pd.concat([merged, info])\n",
        "                print(f\"{future} - {len(info)} records added total records: {len(merged)} \" )\n",
        "\n",
        "            return merged\n",
        "\n",
        "    def fetch(\n",
        "        self,\n",
        "        start: Union[str, datetime],\n",
        "        end: Optional[Union[str, datetime]] = None,\n",
        "        columns: Optional[List[str]] = None,\n",
        "        unvantip: bool = False,\n",
        "    ) -> pd.DataFrame:\n",
        "\n",
        "        start_date_initial = datetime.strptime(start, \"%Y-%m-%d\")\n",
        "        end_date_initial = datetime.strptime(end or start, \"%Y-%m-%d\")\n",
        "\n",
        "        merged = pd.DataFrame()\n",
        "        fonunvantips = [\"\"]\n",
        "        if unvantip :\n",
        "            fonunvantips = self.get_combobox_items(url=\"https://www.tefas.gov.tr/TarihselVeriler.aspx\", select_id=\"DropDownListFundTypeExplanationYAT\")\n",
        "\n",
        "        if self.use_Proxy :\n",
        "            proxy_address = self.get_free_proxy()\n",
        "            self.proxies = {\"http\": proxy_address, \"https\": proxy_address}\n",
        "            print(self.proxies)\n",
        "        else :\n",
        "            self.proxies = None\n",
        "\n",
        "        if self.concurrently :\n",
        "            merged = self.fetch_info_concurrently(fonunvantips, start_date_initial, end_date_initial)\n",
        "        else :\n",
        "            merged = self.fetch_info_serial(fonunvantips, start_date_initial, end_date_initial)\n",
        "\n",
        "        merged = merged[columns] if columns and not merged.empty else merged\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def get_free_proxy(self):\n",
        "        proxy_address = FreeProxy(timeout=1, rand=True, https=True).get()\n",
        "        return proxy_address\n",
        "\n",
        "    def _do_post(self, data: Dict[str, str]) -> Dict[str, str]:\n",
        "        timestamp = int(time.time() * 1000)  # Get current timestamp in milliseconds\n",
        "        headers = {\n",
        "         \"Connection\": \"keep-alive\",\n",
        "         \"Cache-Control\": \"no-cache\",\n",
        "         \"Pragma\": \"no-cache\",\n",
        "         \"X-Requested-With\": \"XMLHttpRequest\",\n",
        "         \"Sec-Fetch-Mode\": \"cors\",\n",
        "         \"Sec-Fetch-Site\": \"same-origin\",\n",
        "         \"Accept-Language\": \"tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
        "         \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
        "         \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
        "         \"Origin\": \"https://www.tefas.gov.tr\",\n",
        "         \"Referer\": f\"https://www.tefas.gov.tr/TarihselVeriler.aspx?timestamp={timestamp}\" ,\n",
        "         }\n",
        "\n",
        "        response = requests.post(\n",
        "             url=f\"{self.root_url}/{self.info_endpoint}\",\n",
        "             data=data,\n",
        "             proxies=self.proxies,\n",
        "             headers=headers,\n",
        "         )\n",
        "        # Check the response status code and content\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Request failed with status code: {response.status_code}\")\n",
        "            print(f\"Response content: {response.text}\")\n",
        "            return {}  # Return an empty dictionary if the request failed\n",
        "        try:\n",
        "            return response.json().get(\"data\", {})\n",
        "        except ValueError as e:\n",
        "            print(f\"Error decoding JSON response: {e}\")\n",
        "            print(f\"Response content: {response.text}\")\n",
        "            return {}\n",
        "\n",
        "    def _parse_date(self, date: Union[str, datetime]) -> str:\n",
        "        if isinstance(date, datetime):\n",
        "            formatted = datetime.strftime(date, \"%d.%m.%Y\")\n",
        "        elif isinstance(date, str):\n",
        "            try:\n",
        "                parsed = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "            except ValueError as exc:\n",
        "                raise ValueError(\n",
        "                    \"Date string format is incorrect. \" \"It should be `YYYY-MM-DD`\"\n",
        "                ) from exc\n",
        "            else:\n",
        "                formatted = datetime.strftime(parsed, \"%d.%m.%Y\")\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"`date` should be a string like 'YYYY-MM-DD' \"\n",
        "                \"or a `datetime.datetime` object.\"\n",
        "            )\n",
        "        return formatted"
      ],
      "metadata": {
        "id": "hFUKdBDKJz1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tefas = tefas_get()\n",
        "\n",
        "time_delta = 450\n",
        "start_date_calc = date.today() - timedelta(days=time_delta)\n",
        "date_start = start_date_calc.strftime(\"%Y-%m-%d\")\n",
        "date_end = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "fetched_data = pd.DataFrame()\n",
        "fetched_data = tefas.fetch(start=date_start, end=date_end, columns=[\"code\", \"date\", \"price\", \"market_cap\", \"number_of_shares\", \"number_of_investors\"], unvantip=False)\n",
        "fetched_data['date'] = pd.to_datetime(fetched_data['date'], errors='coerce')\n",
        "fetched_data['date'].dt.strftime('%Y-%m-%d')\n",
        "fetched_data['date'] = fetched_data['date'].dt.date\n",
        "fetched_data['price'].astype(float,False)\n",
        "fetched_data.rename(columns={'price': 'close'}, inplace=True)\n",
        "fetched_data.rename(columns={'code': 'symbol'}, inplace=True)\n",
        "fetched_data['market_cap'].astype(float,False)\n",
        "fetched_data['number_of_shares'].astype(float,False)\n",
        "fetched_data['number_of_investors'].astype(float,False)\n",
        "fetched_data['market_cap_per_investors'] = fetched_data['market_cap'] / fetched_data['number_of_investors']\n",
        "fetched_data[(fetched_data!=0)&(pd.isnull(fetched_data))]\n",
        "fetched_data = fetched_data.sort_values(['symbol', 'date'])\n",
        "fetched_data['open'] = fetched_data.groupby('symbol')['close'].shift(1)\n",
        "fetched_data['high'] = fetched_data[['open', 'close']].max(axis=1)\n",
        "fetched_data['low'] = fetched_data[['open', 'close']].min(axis=1)\n",
        "fetched_data = fetched_data.dropna()\n",
        "fetched_data.to_csv('tefas.csv', encoding='utf-8-sig', index=False)\n",
        "fetched_data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3IGdd-6deamP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetched_data=pd.read_csv('tefas.csv')\n",
        "fetched_data['date'] = pd.to_datetime(fetched_data['date'])\n",
        "fetched_data['qty'] = 100/fetched_data['close']\n",
        "fetched_data['year'] = fetched_data['date'].dt.year\n",
        "fetched_data['week_no'] = fetched_data['date'].dt.isocalendar().week.astype(str).str.zfill(2)\n",
        "fetched_data['year_week'] = fetched_data['year'].astype(str) +'-'+ fetched_data['week_no'].astype(str)\n",
        "fetched_data['day_of_week'] = fetched_data['date'].apply(lambda x: x.strftime('%A'))\n",
        "idx = fetched_data.groupby(['symbol', 'year_week'])['date'].idxmax()\n",
        "max_prices = fetched_data.loc[idx, ['symbol', 'year_week', 'close']]\n",
        "max_prices = max_prices.rename(columns={'close': 'price_at_week_close'})\n",
        "fetched_data = fetched_data.merge(max_prices, on=['symbol', 'year_week'], how='left')\n",
        "fetched_data['valuation_at_week_close'] = fetched_data['price_at_week_close'] * fetched_data['qty']\n",
        "fetched_data.to_csv('tefas_transformed.csv', encoding='utf-8-sig', index=False)"
      ],
      "metadata": {
        "id": "OiZUKiU4ra0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetched_data=pd.read_csv('tefas_transformed.csv')\n",
        "fetched_data"
      ],
      "metadata": {
        "id": "20j7gHJBgXXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetched_data_agg = tefas.fetch(start=date_end, end=date_end, columns=[\"code\", \"date\", \"price\", \"fonunvantip\", \"title\"], unvantip=True)"
      ],
      "metadata": {
        "id": "ef6rt_omfMCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fon_table = fetched_data_agg.pivot_table(index=['title', 'code'], columns='fonunvantip', aggfunc='size', fill_value=0)\n",
        "fon_table.reset_index(inplace=True)\n",
        "fon_table = fon_table.replace(0, False)\n",
        "fon_table = fon_table.replace(1, True)\n",
        "fon_table.rename(columns={'code': 'symbol'}, inplace=True)\n",
        "fon_table['symbol_with_title'] = fon_table['symbol'].astype(str) +' - '+ fon_table['title'].astype(str)\n",
        "fon_table.to_csv('fon_table.csv', encoding='utf-8-sig', index=False)\n",
        "fon_table"
      ],
      "metadata": {
        "id": "tomLkgsJfSOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}