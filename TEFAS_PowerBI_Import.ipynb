{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUj5fkdjLidWynxBx9Zw2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozturkergin/ozturkergin/blob/main/TEFAS_PowerBI_Import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-YPNguqpIE8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import math\n",
        "import ssl\n",
        "\n",
        "from datetime import datetime, timedelta, date\n",
        "from typing import Dict, List, Optional, Union\n",
        "from marshmallow import Schema, fields, EXCLUDE, pre_load, post_load\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.poolmanager import PoolManager\n",
        "\n",
        "# Special thanks to https://github.com/burakyilmaz321\n",
        "\n",
        "class InfoSchema(Schema):\n",
        "    code = fields.String(data_key=\"FONKODU\", allow_none=True)\n",
        "    date = fields.Date(data_key=\"TARIH\", allow_none=True)\n",
        "    price = fields.Float(data_key=\"FIYAT\", allow_none=True)\n",
        "\n",
        "    @pre_load\n",
        "    def pre_load_hook(self, input_data, **kwargs):\n",
        "        # Convert milliseconds Unix timestamp to date\n",
        "        seconds_timestamp = int(input_data[\"TARIH\"]) / 1000\n",
        "        input_data[\"TARIH\"] = date.fromtimestamp(seconds_timestamp).isoformat()\n",
        "        return input_data\n",
        "\n",
        "    @post_load\n",
        "    def post_load_hool(self, output_data, **kwargs):\n",
        "        # Fill missing fields with default None\n",
        "        output_data = {f: output_data.setdefault(f) for f in self.fields}\n",
        "        return output_data\n",
        "\n",
        "    class Meta:\n",
        "        unknown = EXCLUDE\n",
        "\n",
        "class tefas_get:\n",
        "\n",
        "    root_url = \"https://www.tefas.gov.tr\"\n",
        "    detail_endpoint = \"/api/DB/BindHistoryAllocation\"\n",
        "    info_endpoint = \"/api/DB/BindHistoryInfo\"\n",
        "    headers = {\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"X-Requested-With\": \"XMLHttpRequest\",\n",
        "        \"User-Agent\": (\n",
        "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 \"\n",
        "            \"(KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36\"\n",
        "        ),\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
        "        \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
        "        \"Origin\": \"https://www.tefas.gov.tr\",\n",
        "        \"Referer\": \"https://www.tefas.gov.tr/TarihselVeriler.aspx\",\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = _get_session()\n",
        "        _ = self.session.get(self.root_url)\n",
        "        self.cookies = self.session.cookies.get_dict()\n",
        "\n",
        "    def fetch(\n",
        "        self,\n",
        "        start: Union[str, datetime],\n",
        "        end: Optional[Union[str, datetime]] = None,\n",
        "        name: Optional[str] = None,\n",
        "        columns: Optional[List[str]] = None,\n",
        "    ) -> pd.DataFrame:\n",
        "\n",
        "        start_date_initial = datetime.strptime(start, \"%Y-%m-%d\")\n",
        "        end_date_initial = datetime.strptime(end or start, \"%Y-%m-%d\")\n",
        "        counter = 1\n",
        "        start_date = start_date_initial\n",
        "        end_date = end_date_initial\n",
        "\n",
        "        range_date = end_date_initial - start_date_initial\n",
        "        range_interval = 90\n",
        "\n",
        "        info_schema = InfoSchema(many=True)\n",
        "        merged = pd.DataFrame()\n",
        "\n",
        "        if range_date.days > range_interval :\n",
        "          counter = range_date.days / range_interval\n",
        "          counter = math.ceil(counter)\n",
        "          end_date = start_date + timedelta(days=range_interval)\n",
        "\n",
        "        while counter > 0:\n",
        "          counter -= 1\n",
        "          #print(counter)\n",
        "          #print(start_date)\n",
        "          #print(end_date)\n",
        "\n",
        "          data = {\n",
        "              \"fontip\": \"YAT\",\n",
        "              \"bastarih\": _parse_date(start_date),\n",
        "              \"bittarih\": _parse_date(end_date),\n",
        "              \"fonkod\": name.upper() if name else \"\",\n",
        "          }\n",
        "\n",
        "          # General info pane\n",
        "          info = self._do_post(self.info_endpoint, data)\n",
        "          info = info_schema.load(info)\n",
        "          info = pd.DataFrame(info, columns=info_schema.fields.keys())\n",
        "\n",
        "          merged = pd.concat([merged, info])\n",
        "\n",
        "          # Return only desired columns\n",
        "          merged = merged[columns] if columns else merged\n",
        "\n",
        "          if counter > 0 :\n",
        "            start_date = end_date + timedelta(days=1)\n",
        "            end_date = end_date + timedelta(days=range_interval)\n",
        "            if end_date > end_date_initial :\n",
        "              end_date = end_date_initial\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def _do_post(self, endpoint: str, data: Dict[str, str]) -> Dict[str, str]:\n",
        "        # TODO: error handling. this is quiet fishy now.\n",
        "        response = self.session.post(\n",
        "            url=f\"{self.root_url}/{endpoint}\",\n",
        "            data=data,\n",
        "            cookies=self.cookies,\n",
        "            headers=self.headers,\n",
        "        )\n",
        "        return response.json().get(\"data\", {})\n",
        "\n",
        "def _parse_date(date: Union[str, datetime]) -> str:\n",
        "    if isinstance(date, datetime):\n",
        "        formatted = datetime.strftime(date, \"%d.%m.%Y\")\n",
        "    elif isinstance(date, str):\n",
        "        try:\n",
        "            parsed = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "        except ValueError as exc:\n",
        "            raise ValueError(\n",
        "                \"Date string format is incorrect. \" \"It should be `YYYY-MM-DD`\"\n",
        "            ) from exc\n",
        "        else:\n",
        "            formatted = datetime.strftime(parsed, \"%d.%m.%Y\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"`date` should be a string like 'YYYY-MM-DD' \"\n",
        "            \"or a `datetime.datetime` object.\"\n",
        "        )\n",
        "    return formatted\n",
        "\n",
        "def _get_session() -> requests.Session:\n",
        "\n",
        "    class CustomHttpAdapter(HTTPAdapter):\n",
        "        def __init__(self, ssl_context=None, **kwargs):\n",
        "            self.ssl_context = ssl_context\n",
        "            super().__init__(**kwargs)\n",
        "\n",
        "        def init_poolmanager(\n",
        "            self, connections, maxsize, block=False\n",
        "        ):  # pylint: disable=arguments-differ\n",
        "            self.poolmanager = PoolManager(\n",
        "                num_pools=connections,\n",
        "                maxsize=maxsize,\n",
        "                block=block,\n",
        "                ssl_context=self.ssl_context,\n",
        "            )\n",
        "\n",
        "    ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n",
        "    ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\n",
        "    session = requests.session()\n",
        "    session.mount(\"https://\", CustomHttpAdapter(ctx))\n",
        "    return session\n",
        "\n",
        "time_delta = 365\n",
        "start_date_calc = date.today() - timedelta(days=time_delta)\n",
        "\n",
        "tefas = tefas_get()\n",
        "\n",
        "today_1_year_ago = date.today() - timedelta(days=time_delta)\n",
        "date_start = today_1_year_ago.strftime(\"%Y-%m-%d\")\n",
        "date_end = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "fetched_data = pd.DataFrame()\n",
        "fetched_data = tefas.fetch(start=date_start, end=date_end, columns=[\"code\", \"date\", \"price\"])\n",
        "fetched_data['date'] = pd.to_datetime(fetched_data['date'], errors='coerce')\n",
        "fetched_data['date'].dt.strftime('%Y-%m-%d')\n",
        "fetched_data['price'].astype(float)\n"
      ]
    }
  ]
}